{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halvardbariller/miniconda3/envs/rl_perso/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import gymnasium as gym\n",
    "import gym_pusht\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from IPython.display import Video, Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numba\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from diffusers import DDPMScheduler\n",
    "from diffusers.training_utils import EMAModel\n",
    "from diffusers.optimization import get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/halvardbariller/Documents/gcdp/notebooks\n",
      "Current working directory: /home/halvardbariller/Documents/gcdp/notebooks\n",
      "Python path: ['/home/halvardbariller/miniconda3/envs/rl_perso/lib/python310.zip', '/home/halvardbariller/miniconda3/envs/rl_perso/lib/python3.10', '/home/halvardbariller/miniconda3/envs/rl_perso/lib/python3.10/lib-dynload', '', '/home/halvardbariller/miniconda3/envs/rl_perso/lib/python3.10/site-packages', '/home/halvardbariller/Documents/lerobot', '/home/halvardbariller/miniconda3/envs/rl_perso/lib/python3.10/site-packages/rerun_sdk', '/home/halvardbariller/Documents/gcdp']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "# Change directory to the notebook's directory\n",
    "%cd ~/Documents/gcdp/notebooks\n",
    "\n",
    "# Add the parent directory of 'gcdp' to the Python path\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Verify by printing the current working directory and Python path\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Python path:\", sys.path)\n",
    "\n",
    "# Import the modules\n",
    "from gcdp.episodes import *\n",
    "from gcdp.utils import *\n",
    "from gcdp.diffusion import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    \"gym_pusht/PushT-v0\", obs_type=\"pixels_agent_pos\", render_mode=\"rgb_array\"\n",
    ")\n",
    "env = ScaleRewardWrapper(env)\n",
    "# record_video(env, horizon=180, name=\"PushTRandom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "pred_horizon = 16\n",
    "obs_horizon = 2\n",
    "action_horizon = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch['image'].shape: torch.Size([64, 2, 3, 96, 96])\n",
      "batch['agent_pos'].shape: torch.Size([64, 2, 2])\n",
      "batch['action'].shape torch.Size([64, 16, 2])\n",
      "batch['reached_goal_agent_pos'].shape: torch.Size([64, 2])\n",
      "batch['reached_goal_image'].shape: torch.Size([64, 3, 96, 96])\n"
     ]
    }
   ],
   "source": [
    "trajectories = [\n",
    "    get_rollout(episode_length=i, policy=None, env=env) for i in range(25, 30)\n",
    "]\n",
    "\n",
    "dataset = PushTDatasetFromTrajectories(\n",
    "    trajectories,\n",
    "    pred_horizon=pred_horizon,\n",
    "    obs_horizon=obs_horizon,\n",
    "    action_horizon=action_horizon,\n",
    "    get_original_goal=False,\n",
    ")\n",
    "\n",
    "# create dataloader\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    "    shuffle=True,\n",
    "    # accelerate cpu-gpu transfer\n",
    "    pin_memory=True,\n",
    "    # don't kill worker process afte each epoch\n",
    "    persistent_workers=False,\n",
    ")\n",
    "\n",
    "# visualize data in batch\n",
    "batch = next(iter(dataloader))\n",
    "print(\n",
    "    \"batch['image'].shape:\", batch[\"image\"].shape\n",
    ")  # (batch_size, obs_horizon, C, H, W)\n",
    "print(\n",
    "    \"batch['agent_pos'].shape:\", batch[\"agent_pos\"].shape\n",
    ")  # (batch_size, obs_horizon, 2)\n",
    "print(\n",
    "    \"batch['action'].shape\", batch[\"action\"].shape\n",
    ")  # (batch_size, action_horizon, 2)\n",
    "print(\n",
    "    \"batch['reached_goal_agent_pos'].shape:\",\n",
    "    batch[\"reached_goal_agent_pos\"].shape,\n",
    ")  # (batch_size, 2)\n",
    "print(\n",
    "    \"batch['reached_goal_image'].shape:\", batch[\"reached_goal_image\"].shape\n",
    ")  # (batch_size, C, H, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 8.731597e+07\n"
     ]
    }
   ],
   "source": [
    "# @markdown ### **Network Demo**\n",
    "\n",
    "# construct ResNet18 encoder\n",
    "# if you have multiple camera views, use seperate encoder weights for each view.\n",
    "vision_encoder = get_resnet(\"resnet18\")\n",
    "\n",
    "# IMPORTANT!\n",
    "# replace all BatchNorm with GroupNorm to work with EMA\n",
    "# performance will tank if you forget to do this!\n",
    "vision_encoder = replace_bn_with_gn(vision_encoder)\n",
    "\n",
    "# ResNet18 has output dim of 512\n",
    "vision_feature_dim = 512\n",
    "# agent_pos is 2 dimensional\n",
    "lowdim_obs_dim = 2\n",
    "# observation feature has 514 dims in total per step\n",
    "obs_dim = vision_feature_dim + lowdim_obs_dim\n",
    "action_dim = 2\n",
    "goal_dim = vision_feature_dim + lowdim_obs_dim\n",
    "\n",
    "# create network object\n",
    "noise_pred_net = ConditionalUnet1D(\n",
    "    input_dim=action_dim, global_cond_dim=obs_dim * obs_horizon + goal_dim\n",
    ")\n",
    "\n",
    "# the final arch has 2 parts\n",
    "nets = nn.ModuleDict(\n",
    "    {\"vision_encoder\": vision_encoder, \"noise_pred_net\": noise_pred_net}\n",
    ")\n",
    "\n",
    "# # demo\n",
    "# with torch.no_grad():\n",
    "#     # example inputs\n",
    "#     image = torch.zeros((1, obs_horizon, 3, 96, 96))\n",
    "#     agent_pos = torch.zeros((1, obs_horizon, 2))\n",
    "#     # vision encoder\n",
    "#     image_features = nets[\"vision_encoder\"](image.flatten(end_dim=1))\n",
    "#     # (2,512)\n",
    "#     image_features = image_features.reshape(*image.shape[:2], -1)\n",
    "#     # (1,2,512)\n",
    "#     obs = torch.cat([image_features, agent_pos], dim=-1)\n",
    "#     # (1,2,514)\n",
    "\n",
    "#     noised_action = torch.randn((1, pred_horizon, action_dim))\n",
    "#     diffusion_iter = torch.zeros((1,))\n",
    "\n",
    "#     # the noise prediction network\n",
    "#     # takes noisy action, diffusion iteration and observation as input\n",
    "#     # predicts the noise added to action\n",
    "#     noise = nets[\"noise_pred_net\"](\n",
    "#         sample=noised_action,\n",
    "#         timestep=diffusion_iter,\n",
    "#         global_cond=obs.flatten(start_dim=1),\n",
    "#     )\n",
    "\n",
    "#     # illustration of removing noise\n",
    "#     # the actual noise removal is performed by NoiseScheduler\n",
    "#     # and is dependent on the diffusion noise schedule\n",
    "#     denoised_action = noised_action - noise\n",
    "\n",
    "# for this demo, we use DDPMScheduler with 100 diffusion iterations\n",
    "num_diffusion_iters = 100\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=num_diffusion_iters,\n",
    "    # the choise of beta schedule has big impact on performance\n",
    "    # we found squared cosine works the best\n",
    "    beta_schedule=\"squaredcos_cap_v2\",\n",
    "    # clip output to [-1,1] to improve stability\n",
    "    clip_sample=True,\n",
    "    # our network predicts noise (instead of denoised action)\n",
    "    prediction_type=\"epsilon\",\n",
    ")\n",
    "\n",
    "# device transfer\n",
    "device = torch.device(\"cuda\")\n",
    "_ = nets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Exponential Moving Average\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# accelerates training and improves stability\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# holds a copy of the model weights\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m ema \u001b[38;5;241m=\u001b[39m \u001b[43mEMAModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.75\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Standard ADAM optimizer\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Note that EMA parametesr are not optimized\u001b[39;00m\n\u001b[1;32m     10\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(params\u001b[38;5;241m=\u001b[39mnets\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_perso/lib/python3.10/site-packages/diffusers/training_utils.py:234\u001b[0m, in \u001b[0;36mEMAModel.__init__\u001b[0;34m(self, parameters, decay, min_decay, update_after_step, use_ema_warmup, inv_gamma, power, model_cls, model_config, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m     min_decay \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_value\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    233\u001b[0m parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(parameters)\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshadow_params \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters]\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    237\u001b[0m     deprecation_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `device` argument is deprecated. Please use `to` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_perso/lib/python3.10/site-packages/diffusers/training_utils.py:234\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    231\u001b[0m     min_decay \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_value\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    233\u001b[0m parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(parameters)\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshadow_params \u001b[38;5;241m=\u001b[39m [\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters]\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    237\u001b[0m     deprecation_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `device` argument is deprecated. Please use `to` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "# Exponential Moving Average\n",
    "# accelerates training and improves stability\n",
    "# holds a copy of the model weights\n",
    "ema = EMAModel(parameters=nets.parameters(), power=0.75)\n",
    "\n",
    "# Standard ADAM optimizer\n",
    "# Note that EMA parametesr are not optimized\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=nets.parameters(), lr=1e-4, weight_decay=1e-6\n",
    ")\n",
    "\n",
    "# Cosine LR schedule with linear warmup\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=len(dataloader) * num_epochs,\n",
    ")\n",
    "\n",
    "with tqdm(range(num_epochs), desc=\"Epoch\") as tglobal:\n",
    "    # epoch loop\n",
    "    for epoch_idx in tglobal:\n",
    "        epoch_loss = list()\n",
    "        # batch loop\n",
    "        with tqdm(dataloader, desc=\"Batch\", leave=False) as tepoch:\n",
    "            for nbatch in tepoch:\n",
    "                # data normalized in dataset\n",
    "                # device transfer\n",
    "                nimage = nbatch[\"image\"][:, :obs_horizon].to(device)\n",
    "                nagent_pos = nbatch[\"agent_pos\"][:, :obs_horizon].to(device)\n",
    "                naction = nbatch[\"action\"].to(device)\n",
    "                nreachedimage = nbatch[\"reached_goal_image\"].to(device)\n",
    "                nreachedagent_pos = nbatch[\"reached_goal_agent_pos\"].to(device)\n",
    "                B = nagent_pos.shape[0]\n",
    "\n",
    "                # encoder vision features\n",
    "                image_features = nets[\"vision_encoder\"](\n",
    "                    nimage.flatten(end_dim=1)\n",
    "                )\n",
    "                image_features = image_features.reshape(*nimage.shape[:2], -1)\n",
    "                # (B,obs_horizon,D)\n",
    "\n",
    "                # encoder vision goal\n",
    "                reached_image_features = nets[\"vision_encoder\"](\n",
    "                    nreachedimage.flatten(end_dim=1)\n",
    "                )\n",
    "                reached_image_features = reached_image_features.reshape(\n",
    "                    *nreachedimage.shape[:1], -1\n",
    "                )\n",
    "                # (B,D)\n",
    "\n",
    "                # concatenate vision feature and low-dim obs\n",
    "                obs_features = torch.cat([image_features, nagent_pos], dim=-1)\n",
    "                obs_cond = obs_features.flatten(start_dim=1)\n",
    "                # (B, obs_horizon * obs_dim)\n",
    "\n",
    "                # concatenate vision goal feature and low-dim obs\n",
    "                reached_obs_features = torch.cat(\n",
    "                    [reached_image_features, nreachedagent_pos], dim=-1\n",
    "                )\n",
    "                reached_obs_cond = reached_obs_features.flatten(start_dim=1)\n",
    "\n",
    "                # concatenate obs and goal\n",
    "                full_cond = torch.cat([obs_cond, reached_obs_cond], dim=-1)\n",
    "                # (B, obs_horizon * obs_dim + goal_dim)\n",
    "\n",
    "                # sample noise to add to actions\n",
    "                noise = torch.randn(naction.shape, device=device)\n",
    "\n",
    "                # sample a diffusion iteration for each data point\n",
    "                timesteps = torch.randint(\n",
    "                    0,\n",
    "                    noise_scheduler.config.num_train_timesteps,\n",
    "                    (B,),\n",
    "                    device=device,\n",
    "                ).long()\n",
    "\n",
    "                # add noise to the clean images according to the noise magnitude at each diffusion iteration\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_actions = noise_scheduler.add_noise(\n",
    "                    naction, noise, timesteps\n",
    "                )\n",
    "\n",
    "                # predict the noise residual\n",
    "                noise_pred = noise_pred_net(\n",
    "                    noisy_actions, timesteps, global_cond=full_cond\n",
    "                )\n",
    "\n",
    "                # L2 loss\n",
    "                loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "                # optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                # step lr scheduler every batch\n",
    "                # this is different from standard pytorch behavior\n",
    "                lr_scheduler.step()\n",
    "\n",
    "                # update Exponential Moving Average of the model weights\n",
    "                ema.step(nets.parameters())\n",
    "\n",
    "                # logging\n",
    "                loss_cpu = loss.item()\n",
    "                epoch_loss.append(loss_cpu)\n",
    "                tepoch.set_postfix(loss=loss_cpu)\n",
    "        tglobal.set_postfix(loss=np.mean(epoch_loss))\n",
    "\n",
    "# Weights of the EMA model\n",
    "# is used for inference\n",
    "ema_nets = nets\n",
    "ema.copy_to(ema_nets.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = np.load(\"termination_goal.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(goal[\"pixels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
