{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lerobot\n",
    "from lerobot.common.datasets import lerobot_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a5abf35a66d46c592def674507a659f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 222 files:   0%|          | 0/222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeRobotDataset(\n",
      "  Repository ID: 'lerobot/pusht',\n",
      "  Version: 'v1.4',\n",
      "  Split: 'train',\n",
      "  Number of Samples: 25650,\n",
      "  Number of Episodes: 206,\n",
      "  Type: video (.mp4),\n",
      "  Recorded Frames per Second: 10,\n",
      "  Camera Keys: ['observation.image'],\n",
      "  Video Frame Keys: ['observation.image'],\n",
      "  Transformations: None,\n",
      ")\n",
      "Dataset({\n",
      "    features: ['observation.image', 'observation.state', 'action', 'episode_index', 'frame_index', 'timestamp', 'next.reward', 'next.done', 'next.success', 'index'],\n",
      "    num_rows: 25650\n",
      "})\n",
      "\n",
      "average number of frames per episode: 124.515\n",
      "frames per second used during data collection: dataset.fps=10\n",
      "keys to access images from cameras: dataset.camera_keys=['observation.image']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import imageio\n",
    "import torch\n",
    "# import torch dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import lerobot\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset\n",
    "\n",
    "# print(\"List of available datasets:\")\n",
    "# pprint(lerobot.available_datasets)\n",
    "\n",
    "# Let's take one for this example\n",
    "repo_id = \"lerobot/pusht\"\n",
    "\n",
    "# You can easily load a dataset from a Hugging Face repository\n",
    "dataset = LeRobotDataset(repo_id)\n",
    "\n",
    "# LeRobotDataset is actually a thin wrapper around an underlying Hugging Face dataset\n",
    "# (see https://huggingface.co/docs/datasets/index for more information).\n",
    "print(dataset)\n",
    "print(dataset.hf_dataset)\n",
    "\n",
    "# And provides additional utilities for robotics and compatibility with Pytorch\n",
    "print(f\"\\naverage number of frames per episode: {dataset.num_samples / dataset.num_episodes:.3f}\")\n",
    "print(f\"frames per second used during data collection: {dataset.fps=}\")\n",
    "print(f\"keys to access images from cameras: {dataset.camera_keys=}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access frame indexes associated to first episode\n",
    "episode_index = 0\n",
    "from_idx = dataset.episode_data_index[\"from\"][episode_index].item()\n",
    "to_idx = dataset.episode_data_index[\"to\"][episode_index].item()\n",
    "\n",
    "# LeRobot datasets actually subclass PyTorch datasets so you can do everything you know and love from working\n",
    "# with the latter, like iterating through the dataset. Here we grab all the image frames.\n",
    "frames = [dataset[idx][\"observation.image\"] for idx in range(from_idx, to_idx)]\n",
    "states = [dataset[idx][\"observation.state\"] for idx in range(from_idx, to_idx)]\n",
    "actions = [dataset[idx][\"action\"] for idx in range(from_idx, to_idx)]\n",
    "timestamps = [dataset[idx][\"timestamp\"] for idx in range(from_idx, to_idx)]\n",
    "\n",
    "# Video frames are now float32 in range [0,1] channel first (c,h,w) to follow pytorch convention. To visualize\n",
    "# them, we convert to uint8 in range [0,255]\n",
    "# frames = [(frame * 255).type(torch.uint8) for frame in frames]\n",
    "# # and to channel last (h,w,c).\n",
    "# frames = [frame.permute((1, 2, 0)).numpy() for frame in frames]\n",
    "\n",
    "# # Finally, we save the frames to a mp4 video for visualization.\n",
    "# Path(\"outputs/examples/1_load_lerobot_dataset\").mkdir(parents=True, exist_ok=True)\n",
    "# imageio.mimsave(\"outputs/examples/1_load_lerobot_dataset/episode_0.mp4\", frames, fps=dataset.fps)\n",
    "\n",
    "# # For many machine learning applications we need to load the history of past observations or trajectories of\n",
    "# # future actions. Our datasets can load previous and future frames for each key/modality, using timestamps\n",
    "# # differences with the current loaded frame. For instance:\n",
    "# delta_timestamps = {\n",
    "#     # loads 4 images: 1 second before current frame, 500 ms before, 200 ms before, and current frame\n",
    "#     \"observation.image\": [-1, -0.5, -0.20, 0],\n",
    "#     # loads 8 state vectors: 1.5 seconds before, 1 second before, ... 20 ms, 10 ms, and current frame\n",
    "#     \"observation.state\": [-1.5, -1, -0.5, -0.20, -0.10, -0.02, -0.01, 0],\n",
    "#     # loads 64 action vectors: current frame, 1 frame in the future, 2 frames, ... 63 frames in the future\n",
    "#     \"action\": [t / dataset.fps for t in range(64)],\n",
    "# }\n",
    "# dataset = LeRobotDataset(repo_id, delta_timestamps=delta_timestamps)\n",
    "# print(f\"\\n{dataset[0]['observation.image'].shape=}\")  # (4,c,h,w)\n",
    "# print(f\"{dataset[0]['observation.state'].shape=}\")  # (8,c)\n",
    "# print(f\"{dataset[0]['action'].shape=}\\n\")  # (64,c)\n",
    "\n",
    "# # Finally, our datasets are fully compatible with PyTorch dataloaders and samplers because they are just\n",
    "# # PyTorch datasets.\n",
    "# dataloader = torch.utils.data.DataLoader(\n",
    "#     dataset,\n",
    "#     num_workers=0,\n",
    "#     batch_size=32,\n",
    "#     shuffle=True,\n",
    "# )\n",
    "# for batch in dataloader:\n",
    "#     print(f\"{batch['observation.image'].shape=}\")  # (32,4,c,h,w)\n",
    "#     print(f\"{batch['observation.state'].shape=}\")  # (32,8,c)\n",
    "#     print(f\"{batch['action'].shape=}\")  # (32,64,c)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 96, 96])\n",
      "torch.Size([2])\n",
      "torch.Size([2])\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "for frame, state, action, ts in zip(frames, states, actions, timestamps):\n",
    "    print(frame.shape)\n",
    "    print(state.shape)\n",
    "    print(action.shape)\n",
    "    print(ts)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([222.,  97.])\n",
      "tensor([225.2524,  89.3125])\n",
      "tensor([227.5923,  84.5344])\n",
      "tensor([228.4202,  84.2799])\n",
      "tensor([229.0422,  84.9571])\n",
      "tensor([232.1624,  86.3440])\n",
      "tensor([238.8461,  89.3548])\n",
      "tensor([248.0901,  94.0601])\n",
      "tensor([258.1464,  99.5915])\n",
      "tensor([268.2669, 105.9949])\n",
      "tensor([278.6407, 114.0329])\n",
      "tensor([288.4110, 123.1640])\n",
      "tensor([295.9132, 130.9943])\n",
      "tensor([302.2147, 138.0031])\n",
      "tensor([308.5594, 144.0331])\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    print(states[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'observation.image': [-0.1, 0.0],\n",
       " 'observation.state': [-0.1, 0.0],\n",
       " 'action': [-0.1,\n",
       "  0.0,\n",
       "  0.1,\n",
       "  0.2,\n",
       "  0.3,\n",
       "  0.4,\n",
       "  0.5,\n",
       "  0.6,\n",
       "  0.7,\n",
       "  0.8,\n",
       "  0.9,\n",
       "  1.0,\n",
       "  1.1,\n",
       "  1.2,\n",
       "  1.3,\n",
       "  1.4]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fps = 10\n",
    "obs_steps = 2\n",
    "horizon = 16\n",
    "\n",
    "delta_timestamps = {\n",
    "    \"observation.image\": [i / fps for i in range(1 - obs_steps, 1)],\n",
    "    \"observation.state\": [i / fps for i in range(1 - obs_steps, 1)],\n",
    "    \"action\": [i / fps for i in range(1 - obs_steps, 1 - obs_steps + horizon)],\n",
    "  }\n",
    "\n",
    "delta_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f809f48d7e4ac287a94072bbfaab50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 222 files:   0%|          | 0/222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dataset[0]['observation.image'].shape=torch.Size([2, 3, 96, 96])\n",
      "dataset[0]['observation.state'].shape=torch.Size([2, 2])\n",
      "dataset[0]['action'].shape=torch.Size([16, 2])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = LeRobotDataset(repo_id, delta_timestamps=delta_timestamps)\n",
    "print(f\"\\n{dataset[0]['observation.image'].shape=}\")  # (4,c,h,w)\n",
    "print(f\"{dataset[0]['observation.state'].shape=}\")  # (8,c)\n",
    "print(f\"{dataset[0]['action'].shape=}\\n\")  # (64,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_index = 0\n",
    "from_idx = dataset.episode_data_index[\"from\"][episode_index].item()\n",
    "to_idx = dataset.episode_data_index[\"to\"][episode_index + 10].item()\n",
    "first_rollout = [dataset[idx] for idx in range(from_idx, to_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279\n"
     ]
    }
   ],
   "source": [
    "print(to_idx)\n",
    "# print(first_rollout[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "sub_dataset = CustomDataset(first_rollout)\n",
    "\n",
    "# class FilteredDataset(Dataset):\n",
    "#     def __init__(self, dataset, condition):\n",
    "#         self.dataset = dataset\n",
    "#         self.condition = condition\n",
    "#         self.indices = [i for i, item in enumerate(dataset) if condition(item)]\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.indices)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         original_idx = self.indices[idx]\n",
    "#         return self.dataset[original_idx]\n",
    "\n",
    "# # Define the filtering condition\n",
    "# condition = lambda x: x[\"episode_index\"] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the filter\n",
    "# first_rollout = FilteredDataset(dataset, condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CustomDataset' object has no attribute 'episode_data_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m episode_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 2\u001b[0m from_idx \u001b[38;5;241m=\u001b[39m \u001b[43msub_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepisode_data_index\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom\u001b[39m\u001b[38;5;124m\"\u001b[39m][episode_index]\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CustomDataset' object has no attribute 'episode_data_index'"
     ]
    }
   ],
   "source": [
    "episode_index = 0\n",
    "from_idx = sub_dataset.episode_data_index[\"from\"][episode_index].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "max_horizon= 20\n",
    "action_horizon = 16\n",
    "obs_horizon = 2\n",
    "for j in range(i + 1, max_horizon):\n",
    "    if j > i + action_horizon - obs_horizon + 1:\n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279\n",
      "dict_keys(['observation.image', 'observation.state', 'action', 'episode_index', 'frame_index', 'timestamp', 'next.reward', 'next.done', 'next.success', 'index', 'observation.image_is_pad', 'observation.state_is_pad', 'action_is_pad'])\n"
     ]
    }
   ],
   "source": [
    "print(len(first_rollout))\n",
    "print(first_rollout[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnrichedRobotDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Enriched dataset that includes subsequent goals of the original expert dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, original_dataset, goal_horizon, lerobot_dataset):\n",
    "        \"\"\"Initialize the dataset.\n",
    "\n",
    "        Inputs:\n",
    "            original_dataset: the original dataset to enrich (obtained from LeRobot for example)\n",
    "            goal_horizon: the number of steps to look ahead for the goal (must be at least action_horizon - obs_horizon + 1)\n",
    "        \"\"\"\n",
    "        self.original_dataset = original_dataset\n",
    "        self.enriched_data = []\n",
    "        self.goal_horizon = goal_horizon\n",
    "        self.lerobot_dataset = lerobot_dataset\n",
    "        self._enrich_dataset()\n",
    "\n",
    "    def _enrich_dataset(self):\n",
    "        n = len(self.original_dataset)\n",
    "        for i in range(n):\n",
    "            item = self.original_dataset[i]\n",
    "            agent_pos = item[\"observation.state\"] # (obs_horizon, 2)\n",
    "            action = item[\"action\"] # (action_horizon, 2)\n",
    "            image = item[\"observation.image\"] # (obs_horizon, C, H, W)\n",
    "\n",
    "            episode_index = item[\"episode_index\"]\n",
    "            current_episode_end = self.lerobot_dataset.episode_data_index[\"to\"][episode_index].item()\n",
    "            action_horizon = action.shape[0]\n",
    "            obs_horizon = agent_pos.shape[0]\n",
    "            if self.goal_horizon is None:\n",
    "                self.goal_horizon = current_episode_end\n",
    "            assert self.goal_horizon > action_horizon - obs_horizon + 1, (\n",
    "                f\"goal_horizon ({self.goal_horizon}) must be greater than \"\n",
    "                f\"action_horizon ({action_horizon}) - obs_horizon ({obs_horizon}) + 1\"\n",
    "            )\n",
    "            # iteration at most until the end of the current episode\n",
    "            max_horizon = min(i + self.goal_horizon, current_episode_end)\n",
    "            for j in range(i + 1, max_horizon):\n",
    "                # add future goals at least action_horizon - obs_horizon + 1 steps ahead and beyond\n",
    "                if j > i + action_horizon - obs_horizon + 1:\n",
    "                # print(f\"Enriching dataset: {i}/{n} - {j}/{n}\")\n",
    "                    future_goal_pos = self.original_dataset[j][\"observation.state\"]\n",
    "                    future_goal_image = self.original_dataset[j][\"observation.image\"]\n",
    "                    self.enriched_data.append(\n",
    "                        {\n",
    "                            \"agent_pos\": agent_pos,\n",
    "                            \"action\": action,\n",
    "                            \"image\": image,\n",
    "                            \"reached_goal_agent_pos\": future_goal_pos,\n",
    "                            \"reached_goal_image\": future_goal_image,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of samples in the dataset.\"\"\"\n",
    "        return len(self.enriched_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get a sample from the dataset.\"\"\"\n",
    "        return self.enriched_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "torch.Size([16, 2])\n",
      "torch.Size([2, 3, 96, 96])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 3, 96, 96])\n",
      "3276\n"
     ]
    }
   ],
   "source": [
    "test_dataset = EnrichedRobotDataset(first_rollout, goal_horizon=30, lerobot_dataset=dataset)\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    print(test_dataset[i][\"agent_pos\"].shape)\n",
    "    print(test_dataset[i][\"action\"].shape)\n",
    "    print(test_dataset[i][\"image\"].shape)\n",
    "    print(test_dataset[i][\"reached_goal_agent_pos\"].shape)\n",
    "    print(test_dataset[i][\"reached_goal_image\"].shape)\n",
    "    break\n",
    "\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True],\n",
       "        [True, True]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0][\"reached_goal_agent_pos\"] == sub_dataset[16][\"observation.state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[314.8816, 149.7178],\n",
      "        [321.6607, 155.1989]])\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset[0][\"reached_goal_agent_pos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[228.4202,  84.2799],\n",
      "        [229.0422,  84.9571]])\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset[16][\"agent_pos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_play",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
