{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lerobot\n",
    "from lerobot.common.datasets import lerobot_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0102c0d0439f4562a8e5e1a84cc6fa37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 212 files:   0%|          | 0/212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeRobotDataset(\n",
      "  Repository ID: 'lerobot/pusht',\n",
      "  Split: 'train',\n",
      "  Number of Samples: 25650,\n",
      "  Number of Episodes: 206,\n",
      "  Type: video (.mp4),\n",
      "  Recorded Frames per Second: 10,\n",
      "  Camera Keys: ['observation.image'],\n",
      "  Video Frame Keys: ['observation.image'],\n",
      "  Transformations: None,\n",
      "  Codebase Version: v1.6,\n",
      ")\n",
      "Dataset({\n",
      "    features: ['observation.image', 'observation.state', 'action', 'episode_index', 'frame_index', 'timestamp', 'next.reward', 'next.done', 'next.success', 'index'],\n",
      "    num_rows: 25650\n",
      "})\n",
      "\n",
      "average number of frames per episode: 124.515\n",
      "frames per second used during data collection: dataset.fps=10\n",
      "keys to access images from cameras: dataset.camera_keys=['observation.image']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "# import imageio\n",
    "import torch\n",
    "# import torch dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import lerobot\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset\n",
    "\n",
    "# print(\"List of available datasets:\")\n",
    "# pprint(lerobot.available_datasets)\n",
    "\n",
    "# Let's take one for this example\n",
    "repo_id = \"lerobot/pusht\"\n",
    "\n",
    "# You can easily load a dataset from a Hugging Face repository\n",
    "dataset = LeRobotDataset(repo_id)\n",
    "\n",
    "# LeRobotDataset is actually a thin wrapper around an underlying Hugging Face dataset\n",
    "# (see https://huggingface.co/docs/datasets/index for more information).\n",
    "print(dataset)\n",
    "print(dataset.hf_dataset)\n",
    "\n",
    "# And provides additional utilities for robotics and compatibility with Pytorch\n",
    "print(f\"\\naverage number of frames per episode: {dataset.num_samples / dataset.num_episodes:.3f}\")\n",
    "print(f\"frames per second used during data collection: {dataset.fps=}\")\n",
    "print(f\"keys to access images from cameras: {dataset.camera_keys=}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m to_idx \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mepisode_data_index[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto\u001b[39m\u001b[38;5;124m\"\u001b[39m][episode_index]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# LeRobot datasets actually subclass PyTorch datasets so you can do everything you know and love from working\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# with the latter, like iterating through the dataset. Here we grab all the image frames.\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m frames \u001b[38;5;241m=\u001b[39m [dataset[idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservation.image\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(from_idx, to_idx)]\n\u001b[1;32m      9\u001b[0m states \u001b[38;5;241m=\u001b[39m [dataset[idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservation.state\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(from_idx, to_idx)]\n\u001b[1;32m     10\u001b[0m actions \u001b[38;5;241m=\u001b[39m [dataset[idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(from_idx, to_idx)]\n",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m to_idx \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mepisode_data_index[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto\u001b[39m\u001b[38;5;124m\"\u001b[39m][episode_index]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# LeRobot datasets actually subclass PyTorch datasets so you can do everything you know and love from working\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# with the latter, like iterating through the dataset. Here we grab all the image frames.\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m frames \u001b[38;5;241m=\u001b[39m [\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservation.image\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(from_idx, to_idx)]\n\u001b[1;32m      9\u001b[0m states \u001b[38;5;241m=\u001b[39m [dataset[idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservation.state\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(from_idx, to_idx)]\n\u001b[1;32m     10\u001b[0m actions \u001b[38;5;241m=\u001b[39m [dataset[idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(from_idx, to_idx)]\n",
      "File \u001b[0;32m~/.conda/envs/rl_play/lib/python3.10/site-packages/lerobot/common/datasets/lerobot_dataset.py:148\u001b[0m, in \u001b[0;36mLeRobotDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    139\u001b[0m     item \u001b[38;5;241m=\u001b[39m load_previous_and_future_frames(\n\u001b[1;32m    140\u001b[0m         item,\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhf_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtolerance_s,\n\u001b[1;32m    145\u001b[0m     )\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo:\n\u001b[0;32m--> 148\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[43mload_from_videos\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideo_frame_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideos_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolerance_s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideo_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_transforms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cam \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera_keys:\n",
      "File \u001b[0;32m~/.conda/envs/rl_play/lib/python3.10/site-packages/lerobot/common/datasets/video_utils.py:61\u001b[0m, in \u001b[0;36mload_from_videos\u001b[0;34m(item, video_frame_keys, videos_dir, tolerance_s, backend)\u001b[0m\n\u001b[1;32m     58\u001b[0m         timestamps \u001b[38;5;241m=\u001b[39m [item[key][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m     59\u001b[0m         video_path \u001b[38;5;241m=\u001b[39m data_dir \u001b[38;5;241m/\u001b[39m item[key][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 61\u001b[0m         frames \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_video_frames_torchvision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m         item[key] \u001b[38;5;241m=\u001b[39m frames[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m item\n",
      "File \u001b[0;32m~/.conda/envs/rl_play/lib/python3.10/site-packages/lerobot/common/datasets/video_utils.py:118\u001b[0m, in \u001b[0;36mdecode_video_frames_torchvision\u001b[0;34m(video_path, timestamps, tolerance_s, backend, log_loaded_timestamps)\u001b[0m\n\u001b[1;32m    116\u001b[0m loaded_frames \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    117\u001b[0m loaded_ts \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m reader:\n\u001b[1;32m    119\u001b[0m     current_ts \u001b[38;5;241m=\u001b[39m frame[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpts\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m log_loaded_timestamps:\n",
      "File \u001b[0;32m~/.conda/envs/rl_play/lib/python3.10/site-packages/torchvision/io/video_reader.py:192\u001b[0m, in \u001b[0;36mVideoReader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m         frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m         pts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(frame\u001b[38;5;241m.\u001b[39mpts \u001b[38;5;241m*\u001b[39m frame\u001b[38;5;241m.\u001b[39mtime_base)\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpyav_stream:\n",
      "File \u001b[0;32mav/container/input.pyx:208\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mav/packet.pyx:80\u001b[0m, in \u001b[0;36mav.packet.Packet.decode\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mav/video/stream.pyx:41\u001b[0m, in \u001b[0;36mav.video.stream.VideoStream.decode\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mav/video/stream.pyx:50\u001b[0m, in \u001b[0;36mav.video.stream.VideoStream.decode\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mav/codec/context.pyx:462\u001b[0m, in \u001b[0;36mav.codec.context.CodecContext.decode\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mav/codec/context.pyx:238\u001b[0m, in \u001b[0;36mav.codec.context.CodecContext.open\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mav/error.pyx:326\u001b[0m, in \u001b[0;36mav.error.err_check\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "# Access frame indexes associated to first episode\n",
    "episode_index = 0\n",
    "from_idx = dataset.episode_data_index[\"from\"][episode_index].item()\n",
    "to_idx = dataset.episode_data_index[\"to\"][episode_index].item()\n",
    "\n",
    "# LeRobot datasets actually subclass PyTorch datasets so you can do everything you know and love from working\n",
    "# with the latter, like iterating through the dataset. Here we grab all the image frames.\n",
    "frames = [dataset[idx][\"observation.image\"] for idx in range(from_idx, to_idx)]\n",
    "states = [dataset[idx][\"observation.state\"] for idx in range(from_idx, to_idx)]\n",
    "actions = [dataset[idx][\"action\"] for idx in range(from_idx, to_idx)]\n",
    "timestamps = [dataset[idx][\"timestamp\"] for idx in range(from_idx, to_idx)]\n",
    "\n",
    "# Video frames are now float32 in range [0,1] channel first (c,h,w) to follow pytorch convention. To visualize\n",
    "# them, we convert to uint8 in range [0,255]\n",
    "# frames = [(frame * 255).type(torch.uint8) for frame in frames]\n",
    "# # and to channel last (h,w,c).\n",
    "# frames = [frame.permute((1, 2, 0)).numpy() for frame in frames]\n",
    "\n",
    "# # Finally, we save the frames to a mp4 video for visualization.\n",
    "# Path(\"outputs/examples/1_load_lerobot_dataset\").mkdir(parents=True, exist_ok=True)\n",
    "# imageio.mimsave(\"outputs/examples/1_load_lerobot_dataset/episode_0.mp4\", frames, fps=dataset.fps)\n",
    "\n",
    "# # For many machine learning applications we need to load the history of past observations or trajectories of\n",
    "# # future actions. Our datasets can load previous and future frames for each key/modality, using timestamps\n",
    "# # differences with the current loaded frame. For instance:\n",
    "# delta_timestamps = {\n",
    "#     # loads 4 images: 1 second before current frame, 500 ms before, 200 ms before, and current frame\n",
    "#     \"observation.image\": [-1, -0.5, -0.20, 0],\n",
    "#     # loads 8 state vectors: 1.5 seconds before, 1 second before, ... 20 ms, 10 ms, and current frame\n",
    "#     \"observation.state\": [-1.5, -1, -0.5, -0.20, -0.10, -0.02, -0.01, 0],\n",
    "#     # loads 64 action vectors: current frame, 1 frame in the future, 2 frames, ... 63 frames in the future\n",
    "#     \"action\": [t / dataset.fps for t in range(64)],\n",
    "# }\n",
    "# dataset = LeRobotDataset(repo_id, delta_timestamps=delta_timestamps)\n",
    "# print(f\"\\n{dataset[0]['observation.image'].shape=}\")  # (4,c,h,w)\n",
    "# print(f\"{dataset[0]['observation.state'].shape=}\")  # (8,c)\n",
    "# print(f\"{dataset[0]['action'].shape=}\\n\")  # (64,c)\n",
    "\n",
    "# # Finally, our datasets are fully compatible with PyTorch dataloaders and samplers because they are just\n",
    "# # PyTorch datasets.\n",
    "# dataloader = torch.utils.data.DataLoader(\n",
    "#     dataset,\n",
    "#     num_workers=0,\n",
    "#     batch_size=32,\n",
    "#     shuffle=True,\n",
    "# )\n",
    "# for batch in dataloader:\n",
    "#     print(f\"{batch['observation.image'].shape=}\")  # (32,4,c,h,w)\n",
    "#     print(f\"{batch['observation.state'].shape=}\")  # (32,8,c)\n",
    "#     print(f\"{batch['action'].shape=}\")  # (32,64,c)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 96, 96])\n",
      "torch.Size([2])\n",
      "torch.Size([2])\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "for frame, state, action, ts in zip(frames, states, actions, timestamps):\n",
    "    print(frame.shape)\n",
    "    print(state.shape)\n",
    "    print(action.shape)\n",
    "    print(ts)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([222.,  97.])\n",
      "tensor([225.2524,  89.3125])\n",
      "tensor([227.5923,  84.5344])\n",
      "tensor([228.4202,  84.2799])\n",
      "tensor([229.0422,  84.9571])\n",
      "tensor([232.1624,  86.3440])\n",
      "tensor([238.8461,  89.3548])\n",
      "tensor([248.0901,  94.0601])\n",
      "tensor([258.1464,  99.5915])\n",
      "tensor([268.2669, 105.9949])\n",
      "tensor([278.6407, 114.0329])\n",
      "tensor([288.4110, 123.1640])\n",
      "tensor([295.9132, 130.9943])\n",
      "tensor([302.2147, 138.0031])\n",
      "tensor([308.5594, 144.0331])\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    print(states[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'observation.image': [-0.1, 0.0],\n",
       " 'observation.state': [-0.1, 0.0],\n",
       " 'action': [-0.1,\n",
       "  0.0,\n",
       "  0.1,\n",
       "  0.2,\n",
       "  0.3,\n",
       "  0.4,\n",
       "  0.5,\n",
       "  0.6,\n",
       "  0.7,\n",
       "  0.8,\n",
       "  0.9,\n",
       "  1.0,\n",
       "  1.1,\n",
       "  1.2,\n",
       "  1.3,\n",
       "  1.4]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fps = 10\n",
    "obs_steps = 2\n",
    "horizon = 16\n",
    "\n",
    "delta_timestamps = {\n",
    "    \"observation.image\": [i / fps for i in range(1 - obs_steps, 1)],\n",
    "    \"observation.state\": [i / fps for i in range(1 - obs_steps, 1)],\n",
    "    \"action\": [i / fps for i in range(1 - obs_steps, 1 - obs_steps + horizon)],\n",
    "  }\n",
    "\n",
    "delta_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a955de299f4f989bb70765eaf1a2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 212 files:   0%|          | 0/212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = LeRobotDataset(repo_id, delta_timestamps=delta_timestamps)\n",
    "# print(f\"\\n{dataset[0]['observation.image'].shape=}\")  # (4,c,h,w)\n",
    "# print(f\"{dataset[0]['observation.state'].shape=}\")  # (8,c)\n",
    "# print(f\"{dataset[0]['action'].shape=}\\n\")  # (64,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m from_idx \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mepisode_data_index[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom\u001b[39m\u001b[38;5;124m\"\u001b[39m][episode_index]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m      3\u001b[0m to_idx \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mepisode_data_index[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto\u001b[39m\u001b[38;5;124m\"\u001b[39m][episode_index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m----> 4\u001b[0m first_rollout \u001b[38;5;241m=\u001b[39m [dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(from_idx, to_idx)]\n",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m from_idx \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mepisode_data_index[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom\u001b[39m\u001b[38;5;124m\"\u001b[39m][episode_index]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m      3\u001b[0m to_idx \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mepisode_data_index[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto\u001b[39m\u001b[38;5;124m\"\u001b[39m][episode_index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m----> 4\u001b[0m first_rollout \u001b[38;5;241m=\u001b[39m [\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(from_idx, to_idx)]\n",
      "File \u001b[0;32m~/.conda/envs/rl_play/lib/python3.10/site-packages/lerobot/common/datasets/lerobot_dataset.py:148\u001b[0m, in \u001b[0;36mLeRobotDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    139\u001b[0m     item \u001b[38;5;241m=\u001b[39m load_previous_and_future_frames(\n\u001b[1;32m    140\u001b[0m         item,\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhf_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtolerance_s,\n\u001b[1;32m    145\u001b[0m     )\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo:\n\u001b[0;32m--> 148\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[43mload_from_videos\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideo_frame_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideos_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolerance_s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideo_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_transforms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cam \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera_keys:\n",
      "File \u001b[0;32m~/.conda/envs/rl_play/lib/python3.10/site-packages/lerobot/common/datasets/video_utils.py:61\u001b[0m, in \u001b[0;36mload_from_videos\u001b[0;34m(item, video_frame_keys, videos_dir, tolerance_s, backend)\u001b[0m\n\u001b[1;32m     58\u001b[0m         timestamps \u001b[38;5;241m=\u001b[39m [item[key][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m     59\u001b[0m         video_path \u001b[38;5;241m=\u001b[39m data_dir \u001b[38;5;241m/\u001b[39m item[key][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 61\u001b[0m         frames \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_video_frames_torchvision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m         item[key] \u001b[38;5;241m=\u001b[39m frames[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m item\n",
      "File \u001b[0;32m~/.conda/envs/rl_play/lib/python3.10/site-packages/lerobot/common/datasets/video_utils.py:118\u001b[0m, in \u001b[0;36mdecode_video_frames_torchvision\u001b[0;34m(video_path, timestamps, tolerance_s, backend, log_loaded_timestamps)\u001b[0m\n\u001b[1;32m    116\u001b[0m loaded_frames \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    117\u001b[0m loaded_ts \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m reader:\n\u001b[1;32m    119\u001b[0m     current_ts \u001b[38;5;241m=\u001b[39m frame[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpts\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m log_loaded_timestamps:\n",
      "File \u001b[0;32m~/.conda/envs/rl_play/lib/python3.10/site-packages/torchvision/io/video_reader.py:192\u001b[0m, in \u001b[0;36mVideoReader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m         frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m         pts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(frame\u001b[38;5;241m.\u001b[39mpts \u001b[38;5;241m*\u001b[39m frame\u001b[38;5;241m.\u001b[39mtime_base)\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpyav_stream:\n",
      "File \u001b[0;32mav/container/input.pyx:208\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mav/packet.pyx:80\u001b[0m, in \u001b[0;36mav.packet.Packet.decode\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mav/video/stream.pyx:41\u001b[0m, in \u001b[0;36mav.video.stream.VideoStream.decode\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mav/video/stream.pyx:50\u001b[0m, in \u001b[0;36mav.video.stream.VideoStream.decode\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mav/codec/context.pyx:462\u001b[0m, in \u001b[0;36mav.codec.context.CodecContext.decode\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mav/codec/context.pyx:238\u001b[0m, in \u001b[0;36mav.codec.context.CodecContext.open\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mav/error.pyx:326\u001b[0m, in \u001b[0;36mav.error.err_check\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "episode_index = 0\n",
    "from_idx = dataset.episode_data_index[\"from\"][episode_index].item()\n",
    "to_idx = dataset.episode_data_index[\"to\"][episode_index + 2].item()\n",
    "first_rollout = [dataset[idx] for idx in range(from_idx, to_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420\n"
     ]
    }
   ],
   "source": [
    "print(to_idx)\n",
    "# print(first_rollout[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'first_rollout' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[idx]\n\u001b[0;32m---> 11\u001b[0m sub_dataset \u001b[38;5;241m=\u001b[39m CustomDataset(\u001b[43mfirst_rollout\u001b[49m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# class FilteredDataset(Dataset):\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#     def __init__(self, dataset, condition):\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#         self.dataset = dataset\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# # Define the filtering condition\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# condition = lambda x: x[\"episode_index\"] == 0\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'first_rollout' is not defined"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "sub_dataset = CustomDataset(first_rollout)\n",
    "\n",
    "# class FilteredDataset(Dataset):\n",
    "#     def __init__(self, dataset, condition):\n",
    "#         self.dataset = dataset\n",
    "#         self.condition = condition\n",
    "#         self.indices = [i for i, item in enumerate(dataset) if condition(item)]\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.indices)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         original_idx = self.indices[idx]\n",
    "#         return self.dataset[original_idx]\n",
    "\n",
    "# # Define the filtering condition\n",
    "# condition = lambda x: x[\"episode_index\"] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the filter\n",
    "# first_rollout = FilteredDataset(dataset, condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sub_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m episode_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 2\u001b[0m from_idx \u001b[38;5;241m=\u001b[39m \u001b[43msub_dataset\u001b[49m\u001b[38;5;241m.\u001b[39mepisode_data_index[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom\u001b[39m\u001b[38;5;124m\"\u001b[39m][episode_index]\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sub_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "episode_index = 0\n",
    "from_idx = sub_dataset.episode_data_index[\"from\"][episode_index].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "max_horizon= 20\n",
    "action_horizon = 16\n",
    "obs_horizon = 2\n",
    "for j in range(i + 1, max_horizon):\n",
    "    if j > i + action_horizon - obs_horizon + 1:\n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279\n",
      "dict_keys(['observation.image', 'observation.state', 'action', 'episode_index', 'frame_index', 'timestamp', 'next.reward', 'next.done', 'next.success', 'index', 'observation.image_is_pad', 'observation.state_is_pad', 'action_is_pad'])\n"
     ]
    }
   ],
   "source": [
    "print(len(first_rollout))\n",
    "print(first_rollout[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnrichedRobotDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Enriched dataset that includes subsequent goals of the original expert dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, original_dataset, goal_horizon, lerobot_dataset):\n",
    "        \"\"\"Initialize the dataset.\n",
    "\n",
    "        Inputs:\n",
    "            original_dataset: the original dataset to enrich (obtained from LeRobot for example)\n",
    "            goal_horizon: the number of steps to look ahead for the goal (must be at least action_horizon - obs_horizon + 1)\n",
    "        \"\"\"\n",
    "        self.original_dataset = original_dataset\n",
    "        self.enriched_data = []\n",
    "        self.goal_horizon = goal_horizon\n",
    "        self.lerobot_dataset = lerobot_dataset\n",
    "        self._enrich_dataset()\n",
    "\n",
    "    def _enrich_dataset(self):\n",
    "        n = len(self.original_dataset)\n",
    "        for i in range(n):\n",
    "            item = self.original_dataset[i]\n",
    "            agent_pos = item[\"observation.state\"] # (obs_horizon, 2)\n",
    "            action = item[\"action\"] # (action_horizon, 2)\n",
    "            image = item[\"observation.image\"] # (obs_horizon, C, H, W)\n",
    "\n",
    "            episode_index = item[\"episode_index\"]\n",
    "            current_episode_end = self.lerobot_dataset.episode_data_index[\"to\"][episode_index].item()\n",
    "            action_horizon = action.shape[0]\n",
    "            obs_horizon = agent_pos.shape[0]\n",
    "            if self.goal_horizon is None:\n",
    "                self.goal_horizon = current_episode_end\n",
    "            assert self.goal_horizon > action_horizon - obs_horizon + 1, (\n",
    "                f\"goal_horizon ({self.goal_horizon}) must be greater than \"\n",
    "                f\"action_horizon ({action_horizon}) - obs_horizon ({obs_horizon}) + 1\"\n",
    "            )\n",
    "            # iteration at most until the end of the current episode\n",
    "            max_horizon = min(i + self.goal_horizon, current_episode_end)\n",
    "            for j in range(i + 1, max_horizon):\n",
    "                # add future goals at least action_horizon - obs_horizon + 1 steps ahead and beyond\n",
    "                if j > i + action_horizon - obs_horizon + 1:\n",
    "                # print(f\"Enriching dataset: {i}/{n} - {j}/{n}\")\n",
    "                    future_goal_pos = self.original_dataset[j][\"observation.state\"]\n",
    "                    future_goal_image = self.original_dataset[j][\"observation.image\"]\n",
    "                    self.enriched_data.append(\n",
    "                        {\n",
    "                            \"agent_pos\": agent_pos,\n",
    "                            \"action\": action,\n",
    "                            \"image\": image,\n",
    "                            \"reached_goal_agent_pos\": future_goal_pos,\n",
    "                            \"reached_goal_image\": future_goal_image,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of samples in the dataset.\"\"\"\n",
    "        return len(self.enriched_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get a sample from the dataset.\"\"\"\n",
    "        return self.enriched_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "torch.Size([16, 2])\n",
      "torch.Size([2, 3, 96, 96])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 3, 96, 96])\n",
      "3276\n"
     ]
    }
   ],
   "source": [
    "test_dataset = EnrichedRobotDataset(first_rollout, goal_horizon=30, lerobot_dataset=dataset)\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    print(test_dataset[i][\"agent_pos\"].shape)\n",
    "    print(test_dataset[i][\"action\"].shape)\n",
    "    print(test_dataset[i][\"image\"].shape)\n",
    "    print(test_dataset[i][\"reached_goal_agent_pos\"].shape)\n",
    "    print(test_dataset[i][\"reached_goal_image\"].shape)\n",
    "    break\n",
    "\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True],\n",
       "        [True, True]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0][\"reached_goal_agent_pos\"] == sub_dataset[16][\"observation.state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[314.8816, 149.7178],\n",
      "        [321.6607, 155.1989]])\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset[0][\"reached_goal_agent_pos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[228.4202,  84.2799],\n",
      "        [229.0422,  84.9571]])\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset[16][\"agent_pos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_play",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
